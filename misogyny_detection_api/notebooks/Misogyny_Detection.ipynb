{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1MClNOdWuX-FgwJfaEpI0V-IktsRe4rEn",
      "authorship_tag": "ABX9TyO1jgBvLfoAnqxe4VEv3zWF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubham-Kanse/Misogyny-Detection-with-API-Based-Data-Augmentation/blob/main/Misogyny_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install scikit-learn\n",
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkUrwzBpJBK9",
        "outputId": "15eac894-3761-4d0f-d0d1-d0cd66cbb06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Mysogyny_Detection/data/final_labels.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# --- Column setup ---\n",
        "TEXT_COLUMN = 'body'\n",
        "RAW_LABEL_COLUMN = 'level_1'\n",
        "LABEL_COLUMN = 'misogynistic_binary_label'  # 0 for non-misogynistic, 1 for misogynistic\n",
        "\n",
        "# --- Step 1: Clean missing values ---\n",
        "print(f\"Missing in '{TEXT_COLUMN}': {df[TEXT_COLUMN].isnull().sum()}\")\n",
        "print(f\"Missing in '{RAW_LABEL_COLUMN}': {df[RAW_LABEL_COLUMN].isnull().sum()}\")\n",
        "\n",
        "df[TEXT_COLUMN] = df[TEXT_COLUMN].replace(r'^\\s*$', np.nan, regex=True)\n",
        "df.dropna(subset=[TEXT_COLUMN, RAW_LABEL_COLUMN], inplace=True)\n",
        "\n",
        "# --- Step 2: Check unique label values ---\n",
        "print(\"\\nUnique values in 'level_1':\")\n",
        "print(df[RAW_LABEL_COLUMN].unique())\n",
        "\n",
        "# --- Step 3: Create binary labels ---\n",
        "# Based on actual values in your dataset\n",
        "df[LABEL_COLUMN] = df[RAW_LABEL_COLUMN].apply(\n",
        "    lambda x: 0 if str(x).strip().lower() == 'nonmisogynistic' else 1\n",
        ")\n",
        "\n",
        "# --- Step 4: Confirm label creation ---\n",
        "print(\"\\nBinary Label distribution:\")\n",
        "print(df[LABEL_COLUMN].value_counts())\n",
        "\n",
        "# --- Step 5: Ensure label column is int type ---\n",
        "df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(int)\n",
        "\n",
        "# --- Step 6: Create data splits ---\n",
        "df_train = df[df['split'] == 'train']\n",
        "df_val   = df[df['split'] == 'val']\n",
        "df_test  = df[df['split'] == 'test']\n",
        "\n",
        "# --- Step 7: Show test set distribution ---\n",
        "print(\"\\nTest Set Label Distribution:\")\n",
        "print(df_test[LABEL_COLUMN].value_counts())\n",
        "\n",
        "# --- Optional: Quick check ---\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n",
        "print(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
        "\n",
        "print(\"\\nSample text example from test set:\")\n",
        "print(df_test[[TEXT_COLUMN, RAW_LABEL_COLUMN, LABEL_COLUMN]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibnh5lW4Jhqo",
        "outputId": "fce86a83-a3e7-42a8-9fc6-5a60a3f015cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing in 'body': 12\n",
            "Missing in 'level_1': 0\n",
            "\n",
            "Unique values in 'level_1':\n",
            "['Nonmisogynistic' 'Misogynistic']\n",
            "\n",
            "Binary Label distribution:\n",
            "misogynistic_binary_label\n",
            "0    5856\n",
            "1     699\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test Set Label Distribution:\n",
            "misogynistic_binary_label\n",
            "0    1172\n",
            "1     129\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Total samples: 6555\n",
            "Train: 5254, Val: 0, Test: 1301\n",
            "\n",
            "Sample text example from test set:\n",
            "                                                 body          level_1  \\\n",
            "2   Honestly my favorite thing about this is that ...  Nonmisogynistic   \n",
            "3                Source? Doesnt sound right to me idk  Nonmisogynistic   \n",
            "9                      Isn't this the plot of Cocoon?  Nonmisogynistic   \n",
            "15  Professionals say, that dehydration is caused ...  Nonmisogynistic   \n",
            "17     *I can't believe it's not* virgina spread open  Nonmisogynistic   \n",
            "\n",
            "    misogynistic_binary_label  \n",
            "2                           0  \n",
            "3                           0  \n",
            "9                           0  \n",
            "15                          0  \n",
            "17                          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text) # Ensure text is string\n",
        "\n",
        "    # Emoji decoding\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \")) # \"😃\" -> \" smiling_face_with_big_eyes \"\n",
        "\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Simple cleaning: remove URLs, mentions (if any), special characters (optional, BERT can handle some)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
        "    text = re.sub(r'\\@\\w+', '', text) # Remove mentions\n",
        "    text = re.sub(r'#', '', text) # Remove hashtag symbol but keep the text\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation (optional, consider if it helps your specific dataset)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "df['processed_text'] = df[TEXT_COLUMN].apply(preprocess_text)\n",
        "\n",
        "print(\"\\nSample of processed text:\")\n",
        "print(df['processed_text'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDZ1Eo0oNdru",
        "outputId": "c2de0c3e-3529-49c0-f3e4-f5bdf8ecdf0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample of processed text:\n",
            "0    do you have the skin of a 80 year old grandma ...\n",
            "1    this is taking a grain of truth and extrapolat...\n",
            "2    honestly my favorite thing about this is that ...\n",
            "3                  source doesnt sound right to me idk\n",
            "4    damn i saw a movie in which the old woman bath...\n",
            "Name: processed_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "# We'll fit it on the non-misogynistic and misogynistic content separately if we want to find class-specific keywords,\n",
        "# or on all content if we want general keywords for each document.\n",
        "# For augmenting each document, we'll fit on the current document or a small batch.\n",
        "# Here, let's extract keywords for each document individually.\n",
        "# This is a simplification; the paper might imply a more global TF-IDF for identifying\n",
        "\n",
        "def extract_keywords_for_doc(doc_text, top_n=5):\n",
        "    if not doc_text.strip(): # Handle empty strings\n",
        "        return []\n",
        "    try:\n",
        "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_vectorizer.fit([doc_text]) # Fit on the single document\n",
        "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "        tfidf_scores = tfidf_vectorizer.transform([doc_text]).toarray().flatten()\n",
        "        sorted_indices = tfidf_scores.argsort()[::-1]\n",
        "        keywords = [feature_names[i] for i in sorted_indices[:top_n] if tfidf_scores[i] > 0]\n",
        "        return keywords\n",
        "    except ValueError: # Happens if doc_text is empty after stopword removal\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "OOmQF4EgNf8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load the Sentence-Transformer model\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def contextual_expansion(text, keywords, model, top_k_similar=1):\n",
        "    \"\"\"\n",
        "    Expands text by finding terms semantically similar to its keywords.\n",
        "    This is a simplified interpretation of the paper's \"contextual expansion\".\n",
        "    The paper mentions \"retrieve semantically related words and phrases\" [cite: 134]\n",
        "    and \"identifies related misogynistic concepts\"[cite: 136].\n",
        "\n",
        "    A more direct approach for augmentation might be paraphrasing or synonym replacement.\n",
        "    Here, we find similar words to the keywords and append them.\n",
        "    This is a challenging step to replicate exactly without more details on how\n",
        "    \"related concepts\" were integrated.\n",
        "\n",
        "    Let's try a simple approach: for each keyword, find a similar term from a predefined\n",
        "    candidate list or by perturbing the keyword slightly and finding similar phrases.\n",
        "    Given the paper talks about \"discrimination against women\" as a related concept[cite: 136],\n",
        "    it suggests a knowledge base or a way to generate candidates.\n",
        "\n",
        "    Alternative simple augmentation: For each keyword, we could try to find a highly similar\n",
        "    word from a general vocabulary, or just use the keywords themselves to reinforce the text.\n",
        "    The paper's description is: \"The model then identifies related misogynistic concepts...\n",
        "    based on the semantic proximity of their embeddings\"[cite: 136].\n",
        "\n",
        "    Let's try to augment by adding keywords themselves, as a very basic first step,\n",
        "    and then consider how to get \"related concepts\". A full replication of finding\n",
        "    \"sexism\", \"discrimination against women\" from \"misogyny\" [cite: 135, 136] via embeddings alone\n",
        "    without a target corpus of concepts is non-trivial.\n",
        "\n",
        "    A practical approach for augmenting text for BERT:\n",
        "    If we have keywords, we can append them. For more sophisticated augmentation,\n",
        "    techniques like back-translation or using a thesaurus with embeddings are common.\n",
        "    The paper's phrasing \"feed the model with feature patterns\" [cite: 131] might mean\n",
        "    the keywords themselves are the \"focused set of terms\" for the sentence transformer\n",
        "    to create an embedding of the *context* of those keywords, and then similar *sentences*\n",
        "    or *paraphrases* are generated/retrieved.\n",
        "\n",
        "    Given the complexity and potential ambiguity in replicating the exact augmentation\n",
        "    that led to perfect/near-perfect scores[cite: 285, 280], we will implement a simplified\n",
        "    version where we append the extracted keywords to the text. This boosts the signal\n",
        "    of these important terms for BERT.\n",
        "    \"\"\"\n",
        "    if not keywords:\n",
        "        return text\n",
        "    expanded_text = text + \" \" + \" \".join(keywords)\n",
        "    return expanded_text\n",
        "\n",
        "\n",
        "df['keywords'] = df['processed_text'].apply(lambda x: extract_keywords_for_doc(x, top_n=3))\n",
        "df['augmented_text'] = df.apply(lambda row: contextual_expansion(row['processed_text'], row['keywords'], sentence_model), axis=1)\n",
        "\n",
        "print(\"\\nSample of augmented text:\")\n",
        "print(df[['processed_text', 'keywords', 'augmented_text']].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLqt16ZJNjll",
        "outputId": "dc220c7b-accc-40ae-8314-48cf8a374b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample of augmented text:\n",
            "                                      processed_text                 keywords  \\\n",
            "0  do you have the skin of a 80 year old grandma ...     [year, worry, water]   \n",
            "1  this is taking a grain of truth and extrapolat...   [youll, truth, taking]   \n",
            "2  honestly my favorite thing about this is that ...    [water, thing, prove]   \n",
            "3                source doesnt sound right to me idk   [source, sound, right]   \n",
            "4  damn i saw a movie in which the old woman bath...  [woman, water, virgins]   \n",
            "\n",
            "                                      augmented_text  \n",
            "0  do you have the skin of a 80 year old grandma ...  \n",
            "1  this is taking a grain of truth and extrapolat...  \n",
            "2  honestly my favorite thing about this is that ...  \n",
            "3  source doesnt sound right to me idk source sou...  \n",
            "4  damn i saw a movie in which the old woman bath...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Initialize BERT Tokenizer\n",
        "MODEL_NAME = 'bert-base-uncased' # Or any other BERT model you prefer from the paper like RoBERTa, DistilBERT [cite: 175]\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Texts and Labels\n",
        "texts = df['augmented_text'].tolist()\n",
        "labels = df[LABEL_COLUMN].tolist()\n",
        "\n",
        "# Ensure labels are integers\n",
        "labels = [int(label) for label in labels]\n",
        "\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# Tokenize the texts\n",
        "# The paper uses a max sequence length. BERT has a limit (typically 512).\n",
        "# Let's find a suitable max_length or truncate.\n",
        "# For now, let's set a common max_length.\n",
        "MAX_LENGTH = 128 # Adjust based on your data's text length distribution\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
        "\n",
        "print(f\"\\nShape of training input_ids: {train_encodings['input_ids'].shape}\")\n",
        "print(f\"Shape of validation input_ids: {val_encodings['input_ids'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCpF_qvmNxgN",
        "outputId": "76a5a823-2aa3-4e9e-a403-110cd37b112c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of training input_ids: torch.Size([5244, 128])\n",
            "Shape of validation input_ids: torch.Size([1311, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class MisogynyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # BERT for sequence classification expects 'labels'\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = MisogynyDataset(train_encodings, train_labels)\n",
        "val_dataset = MisogynyDataset(val_encodings, val_labels)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) # Adjust batch_size based on your GPU memory\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "X-boEHXlN17h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Load BERT model for sequence classification\n",
        "# num_labels should be 2 for binary classification (misogynistic vs. non-misogynistic)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Set up training parameters\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5) # Learning rate, 5e-5 is a common default for BERT\n",
        "\n",
        "num_epochs = 3 # The paper doesn't specify epochs, 2-4 is common for fine-tuning\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training loop\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        if (batch_idx + 1) % 50 == 0: # Print progress every 50 batches\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1} finished. Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation step (optional, but good practice)\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            val_preds.extend(predictions.cpu().numpy())\n",
        "            val_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(val_true_labels, val_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(val_true_labels, val_preds, average='binary') # Use 'binary' for 2 classes\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQuzKOJ9N4BY",
        "outputId": "b064bcda-47f7-402e-97db-84a1ea65c0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "Epoch 1/3, Batch 50/328, Loss: 0.0969\n",
            "Epoch 1/3, Batch 100/328, Loss: 0.1250\n",
            "Epoch 1/3, Batch 150/328, Loss: 0.4716\n",
            "Epoch 1/3, Batch 200/328, Loss: 0.2407\n",
            "Epoch 1/3, Batch 250/328, Loss: 0.2333\n",
            "Epoch 1/3, Batch 300/328, Loss: 0.3681\n",
            "Epoch 1 finished. Average Training Loss: 0.2800\n",
            "Validation Accuracy: 0.9130, Precision: 0.9333, Recall: 0.2000, F1: 0.3294\n",
            "Epoch 2/3, Batch 50/328, Loss: 0.1144\n",
            "Epoch 2/3, Batch 100/328, Loss: 0.3817\n",
            "Epoch 2/3, Batch 150/328, Loss: 0.1785\n",
            "Epoch 2/3, Batch 200/328, Loss: 0.0093\n",
            "Epoch 2/3, Batch 250/328, Loss: 0.3472\n",
            "Epoch 2/3, Batch 300/328, Loss: 0.3843\n",
            "Epoch 2 finished. Average Training Loss: 0.1576\n",
            "Validation Accuracy: 0.9275, Precision: 0.7184, Recall: 0.5286, F1: 0.6091\n",
            "Epoch 3/3, Batch 50/328, Loss: 0.4282\n",
            "Epoch 3/3, Batch 100/328, Loss: 0.5315\n",
            "Epoch 3/3, Batch 150/328, Loss: 0.0087\n",
            "Epoch 3/3, Batch 200/328, Loss: 0.0009\n",
            "Epoch 3/3, Batch 250/328, Loss: 0.0147\n",
            "Epoch 3/3, Batch 300/328, Loss: 0.0199\n",
            "Epoch 3 finished. Average Training Loss: 0.0660\n",
            "Validation Accuracy: 0.9252, Precision: 0.6842, Recall: 0.5571, F1: 0.6142\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation has been partially done in the training loop's validation step.\n",
        "# Here's a more formal evaluation function:\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions_list = []\n",
        "    true_labels_list = []\n",
        "    probabilities_list = [] # For ROC AUC\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            probs = torch.softmax(logits, dim=-1)[:, 1] # Probability of the positive class (class 1)\n",
        "\n",
        "            predictions_list.extend(preds.cpu().numpy())\n",
        "            true_labels_list.extend(labels.cpu().numpy())\n",
        "            probabilities_list.extend(probs.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels_list, predictions_list)\n",
        "    # For binary classification, specify pos_label=1 if your positive class is 1\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels_list, predictions_list, average='binary', pos_label=1, zero_division=0)\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(true_labels_list, probabilities_list)\n",
        "    except ValueError as e:\n",
        "        print(f\"ROC AUC calculation error: {e}. This might happen if only one class is present in true labels during a batch or small evaluation set.\")\n",
        "        roc_auc = 0.0 # Or handle as appropriate\n",
        "\n",
        "    print(f\"\\n--- Final Evaluation Metrics ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f} [cite: 230]\")\n",
        "    print(f\"Recall: {recall:.4f} [cite: 231]\")\n",
        "    print(f\"F1-Score: {f1:.4f} [cite: 234]\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f} [cite: 238]\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1,\n",
        "        \"roc_auc\": roc_auc\n",
        "    }\n",
        "\n",
        "# Evaluate on the validation set\n",
        "evaluation_results = evaluate_model(model, val_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xg6kr2yPdQM",
        "outputId": "b36df7e8-7a00-45a4-9416-2f3a676be7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation Metrics ---\n",
            "Accuracy: 0.9252\n",
            "Precision: 0.6842 [cite: 230]\n",
            "Recall: 0.5571 [cite: 231]\n",
            "F1-Score: 0.6142 [cite: 234]\n",
            "ROC AUC: 0.8880 [cite: 238]\n"
          ]
        }
      ]
    }
  ]
}